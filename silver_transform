import dlt
from pyspark.sql.functions import col, from_json
from pyspark.sql.types import StructType, StructField, StringType

#define our schema
json_schema = StructType([
    StructField("id", StringType()),
        ])
    
@dlt.table(
    name="silver_cleaned_events", #name of silver table
    comment="Cleaned and enriched events from bronze layer",
    table_properties={
        "quality": "silver" #silver of medallion
    }
)
#quality validation
# @dlt.expect("non_null_event_type", "event_type IS NOT NULL")
def silver_table():

    # Apply your reusable transformation logic    
    df = dlt.read_stream("bronze_kafka_stream").select(from_json(col("raw_json"), json_schema).alias("data")).select("data.*")
    
    return df
